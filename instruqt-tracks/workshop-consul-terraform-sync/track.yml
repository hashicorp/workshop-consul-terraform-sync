slug: workshop-consul-terraform-sync
id: zd9iqr8wyub0
version: 0.0.1
type: track
title: 'Workshop: Consul-Terraform-Sync'
teaser: Use Terraform and Consul to manage day one and day two operations of F5 in
  Azure
description: Use Terraform, Consul, and Consul Terraform Sync to manage day one and
  day two operations of F5 BIG-IPs and Palo Alto Firewalls in Azure.
icon: ""
tags:
- consul
- NIA
owner: hashicorp
developers:
- lance@hashicorp.com
- npearce@hashicorp.com
private: true
published: true
show_timer: true
skipping_enabled: true
challenges:
- slug: 1-review-lab-objectives
  id: rihjzbfzsm9m
  type: challenge
  title: Review Lab Objectives
  teaser: Before we begin, lets quickly review the architecture.
  assignment: |-
    In the next few challenges we are going to provision resources to build the cloud environment for Consul Terraform Sync.

    Once the environment has finished provisioning, you will install `consul-terraform-sync` and review its configuration and workflow.
  notes:
  - type: text
    contents: Every journey begins with a single step! Step 1, review the environment
      Terraform is going to build for Consul Terraform Sync.
  tabs:
  - title: Before CTS - Manual
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C1-Before_CTS.html
  - title: After CTS - Automated
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C1-With_CTS.html
  difficulty: basic
  timelimit: 300
- slug: 2-provision-azure-vnets
  id: y1cns7c1xooc
  type: challenge
  title: Provision Azure VNETs
  teaser: Deploy basic network infrastructure using Terraform
  assignment: |-
    In this assignment you will provision the VNets we will use in the following assignments. <br>

    Inspect and deploy the Terraform code.

    In the `Shell` tab run the following commands.
    ```
    terraform plan
    terraform apply -auto-approve
    ```

    Their CIDR blocks are listed below:
    ```
    hcs-vnet: 10.0.0.0/16
    shared-svcs-vnet: 10.2.0.0/16
    app-vnet: 10.3.0.0/16
    ```

    app-vnet subnets:
    ```
    MGMT: 10.3.1.0/24
    INTERNET: 10.3.2.0/24
    DMZ: 10.3.3.0/24
    APP: 10.3.4.0/24
    ```

    You will leverage these VNet in the next few assignments.
  notes:
  - type: text
    contents: |
      Setting up your environment... Your Azure account will be ready in ~5 minutes.
      Keep an eye on the bottom right corner to know when you can get started.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Text Editor
    type: code
    hostname: workstation
    path: /root/terraform/vnet
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C2-Provision_VNETs.html
  difficulty: basic
  timelimit: 3000
- slug: 3-provision-core-services
  id: 8g5r3gchzqun
  type: challenge
  title: Provision Core Services
  teaser: Provision Vault and HCS using Terraform
  assignment: "You will use Terraform to provision these services in the background
    while you set up Consul in the next few assignments. <br>\n\nStart with Vault.
    Vault is a secrets management solution that we will use to securely store sensitive
    information such as usernames, passwords, certificates, and tokens.<br>\n\nIn
    the `Shell` tab run the following commands.\n\n```\ncd /root/terraform/vault\nterraform
    plan\nterraform apply -auto-approve\n```\n\nNow provision HCS:\n  \nIn the `Shell`
    tab run the following commands.\n\n```\ncd /root/terraform/hcs\nterraform plan\nnohup
    terraform apply -auto-approve  > /root/terraform/hcs/terraform.out &\n```"
  notes:
  - type: text
    contents: |
      Terraform allows you to document, share, and deploy environments in one workflow by using Infrastructure as Code!
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Vault Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/vault
  - title: HCS Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/hcs
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C3-Provision_Core_Svcs.html
  difficulty: basic
  timelimit: 3000
- slug: 4-provision-network-infra
  id: fbyphtsxladi
  type: challenge
  title: Provision F5 BIG-IP & Palo Alto Firewall
  teaser: Provision an F5 BIG-IP VE & Palo Alto Firewall using Terraform
  assignment: "First we will provision the Palo Alto Firewall.\n <br>\n\nIn the `Shell`
    tab run the following commands.\n```\ncd /root/terraform/panw-vm\nterraform plan\nnohup
    terraform apply -auto-approve > /root/terraform/panw-vm/terraform.out &\n```\n\n
    Now we will provision the F5 BIG-IP Virtual Edition using Terraform. <br>\n\nIn
    the `Shell` tab run the following commands.\n ```\ncd /root/terraform/bigip\nterraform
    plan\nterraform apply -auto-approve\n```\n\n This can take several minutes to
    complete, while you are waiting\ntake the opportunity to review the `Terraform
    Code` tab to see the IaC definition.\n\nOnce the apply is complete, you can Navigate
    to the BIG-IP at the IP address in the Terraform output.\n\n**NOTE:** you will
    need open the URL provided by the output in a separate tab.  If you are using
    chrome, you\nmay be presented with a certificate error, to bypass this you can
    type \"thisisunsafe\" into the Chrome window.\n\nThe HCS services may not be running
    quite yet, but are required before we move on to the next challenge. \nYou can
    monitor its progress with the following command:\n\nMontior HCS provisioning\n```\ntail
    -f /root/terraform/hcs/terraform.out\n```"
  notes:
  - type: text
    contents: |
      In this exercise we will be provisioning an F5 BIG-IP Virtual Edition using Terraform.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/bigip
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/panw-vm
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C4-Provision_BIG-IP.html
  difficulty: basic
  timelimit: 3000
- slug: 5-configure-palo-alto-firewall
  id: fyfegnx6ag8o
  type: challenge
  title: Configure Palo Alto Firewall
  teaser: Provision a Palo Alto VM-Series Firewall using Terraform.
  assignment: "Now we will configure the Palo Alto Firewall using Terraform.\n\n First,
    let's verify that the Palo Alto VM Series has been instanciated.\n In the `Shell`
    tab run the following commands.\n\n ```\ntail /root/terraform/panw-vm/terraform.out\n```\n\n
    You should see `Apple complete!`. If so, proceed with the following commands:\n\n
    ```\nterraform plan\nterraform apply -auto-approve\n```\n\nNow we must instrut
    the firewall to `commit` this change\n```\n/root/terraform/panos_commit/panos-commit
    -config /root/terraform/panos_commit/panos-commit.json\n```\n\nThis can take several
    minutes to complete, while you are waiting \ntake the opportunity to review the
    `Terraform Code` tab to see the\nIaC definition.\n\nOnce the apply is complete,
    you can Navigate to the Palo Alto Firewall\nat the IP address in the Terraform
    output.\n\nFor CLI access:\n ```\nssh -q -A -J azure-user@$bastion_ip paloalto@$firewall_ip\n```"
  notes:
  - type: text
    contents: Replace this text with your own text
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/panw-config
  - title: App Access Info
    type: code
    hostname: workstation
    path: /info.txt
  - title: Provision PANW
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C5-Provision_PANW_FW.html
  difficulty: basic
  timelimit: 300
- slug: 6-validate-hcs
  id: z5sy2diwt2hl
  type: challenge
  title: Validate HCS
  teaser: Verify Vault, HCS, and Consul are operational
  assignment: |
    Consul HCS and Vault should now be provisioned and accessible from the corresponding tabs.

    In this exercise we will gather the information required to connect to HCS and securely store this information in Vault.

    In the `Shell` tab run the following commands.
    ```
    vault login -method=userpass username=operations password=Password1
    ```

    Retrieve the bootstrap token and gossip key from HCS and save it to your Vault instance.

    ```
    echo $CONSUL_HTTP_ADDR
    echo $VAULT_ADDR
    bootstrap_token=$(az hcs create-token --resource-group $(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name) --name hcs | jq  -r .masterToken.secretId)
    gossip_key=$(az resource show --ids "/subscriptions/$(az account show | jq -r .id)/resourceGroups/$(terraform output -state /root/terraform/vnet/terraform.tfstate resource_group_name)/providers/Microsoft.Solutions/applications/hcs/customconsulClusters/hashicorp-consul-cluster" --api-version 2018-09-01-preview | jq -r .properties.consulConfigFile | base64 -d | jq -r .encrypt)
    vault kv put secret/consul master_token=${bootstrap_token} gossip_key=${gossip_key}
    ```

    Now inspect the credentials.

    ```
    echo $VAULT_ADDR
    vault kv get secret/consul
    ```
    You can use this token to login and explore the Consul UI, use of the master token should be highly restricted, instead let's configure Vault to issue [dynamic secrets](https://www.vaultproject.io/docs/secrets/consul/) for Consul. <br>

    Get a management token for Vault to manage Consul tokens with.
    You can retrieve the privileged token for this operation from Vault.  <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault kv get -field=master_token secret/consul)
    vault_consul_mgmt_token=$(consul acl token create -policy-name=global-management -description "vault mgmt" | grep SecretID | cut -d ":" -f2 | xargs)

    ```
    Now configure the secrets engine.

    ```
    vault write consul/config/access address=${CONSUL_HTTP_ADDR} token=${vault_consul_mgmt_token}
    vault read consul/config/access
    ```

    Last, create a policy for the operations team, and link it to the Vault role.

    ```
    consul acl policy create -name "ops" -description "admin policy for ops" -rules 'acl = "write" operator = "write" namespace_prefix "" {acl = "write"}'
    vault write consul/roles/ops policies=ops ttl=1h
    ```

    Now you are ready to get a dynamic Consul token from Vault for an operator.
    Validate the token after you fetch it. <br>

    ```
    export CONSUL_HTTP_TOKEN=$(vault read -field token consul/creds/ops)
    consul acl token read -self
    ```

    You can use this token to set up the anonymous policy.

    ```
    echo '
    node_prefix "" {
      policy = "read"
    }
    service_prefix "" {
      policy = "read"
    }
    session_prefix "" {
      policy = "read"
    }
    agent_prefix "" {
      policy = "read"
    }
    query_prefix "" {
      policy = "read"
    }
    operator = "read"' |  consul acl policy create -name anonymous -rules -
    consul acl token update -id anonymous -policy-name anonymous
    ```

    You will use this role in a later assignment to configure access for Consul service consumers.
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: Text Editor
    type: code
    hostname: workstation
    path: /root
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C5-Provision_PANW_FW.html
  difficulty: basic
  timelimit: 3000
- slug: 7-deploy-app-environments
  id: k0reagnnfzuc
  type: challenge
  title: Deploy App environments
  teaser: Deploy a 2-tier VM based application to the cloud.
  assignment: "\nIn this assignment we will be deploying the current application into
    Azure based VM's. <br>\n\nAs part of the cloud migration, the VM's will also be
    configured to run a Consul agent that registers these services with Consul.  This
    will make it easy to refactor the application, as the application is no longer
    dependent upon static IP addresses which are hardcoded into configuration and
    application code. <br>\n\nThis also means that IP addresses no longer have to
    be known before provisioning can occur, thus, decoupling steps in the provisioning
    workflow.\n\n\nReview the code in the `Terraform Code` this defines the VMSS for
    the web and app tiers of the application.\n\nBegin provisioning the application
    in the background.\n\n```\nterraform plan\nnohup terraform apply -auto-approve
    > deploy_app.log 2>&1 &\n```\n\nBy registering nodes and services in Consul other
    services can easily discover their status and location. <br>\n\nYou can now monitor
    the progress of the App deployment by watching the `Consul` tab as the new App
    and Web services auto-register.\n\nYou could also watch the output of the app
    deployment log by executing the following command in the `Shell` tab \n```\ntail
    -f /root/terraform/app/deploy_app.log\n```\n\n You will explore the environment
    in more detail in the next challange. <br>"
  notes:
  - type: text
    contents: |
      For a lot of organizations digital transformation may start with a simple "lift and shift" to the cloud for existing workloads!
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/app
  - title: App Access Info
    type: code
    hostname: workstation
    path: /info.txt
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/workshop-consul-terraform-sync/main/instruqt-tracks/workshop-consul-terraform-sync/assets/images/CTS_Workshop_C7-Provision_App.html
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: App
    type: service
    hostname: workstation
    path: /ui
    port: 8080
  difficulty: basic
  timelimit: 3000
- slug: 8-install-consul-terraform-sync
  id: jskeddctb13s
  type: challenge
  title: Deploy 'Consul Terraform Sync'
  teaser: Now we are going install Consul Terraform Sync and bring dynamic service
    discovery to network infrastructure.
  assignment: |2-

    ```
    terraform plan
    terraform apply -auto-approve
    ```

    When complete, connect to the image
    ```
    consul_terraform_sync=$(curl -s $CONSUL_HTTP_ADDR/v1/catalog/node/consul-terraform-sync | jq -r '.Node.Address')
    ssh -q -A -J azure-user@$bastion_ip azure-user@$consul_terraform_sync
    ```

    Take a look at the `consul-terraform-sync` configuration

    ```
    cat /etc/consul-tf-sync.d/consul-tf-sync.hcl
    ```

    Review the terraform-consul-sync service status with the following command
    ```
    service consul-tf-sync status
    ```

    Move to the next assignment when they are ready.
  notes:
  - type: text
    contents: |
      Now we're going to eliminate a lot of operational friction by installing `consul-terraform-sync`. This will auto-update network infrastructure as service changes occur!
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/consul-tf-sync
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/f5-hashicorp-app-mod-tf-consul/assets/diagrams/final-architecture.html
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: Shell
    type: terminal
    hostname: workstation
  - title: App
    type: service
    hostname: workstation
    path: /ui
    port: 8080
  - title: App Access Info
    type: code
    hostname: workstation
    path: /info.txt
  difficulty: basic
  timelimit: 3000
- slug: 9-review-app-environment
  id: hbbtrye3yi1b
  type: challenge
  title: Review app environment
  teaser: Review the components of the app application
  assignment: |-
    The VM based application is now configured as part of a VM scale set on Azure.  One of the first challenges that comes up is that instances of the scale set will be provisioned with dynamic IP addresses. <br>

    This makes it difficult to maintain any configuration files which require hard coded IP addresses, luckily Consul can help solve this.

    Consul maintains a real-time catalog of all of the nodes and services in the environment.  This catalog can be queried using a UI, CLI, API, or simple DNS interface.

    Let's review the components of the application and see how Consul is leveraged in different ways.

    ** F5 LTM ** <br>

    **Remember you will to need to view this UI in a different browser tab**, for convenenience we have put all the required information in the `info.txt` file in the `App Access Info` tab <br>

    The VIP and pools are provisioned in the `consul-terraform-sync` partition, make sure you select it from the drop down menu in the top right corner of the screen.

    You can view the VIP in the BIG-IP UI by naviagating to the `Local Traffic -> Virtual Servers`

    You can view the pool members navigating to `Local Traffic -> Pools`


    ** Web Tier ** <br>

    Company policy is to have ingress communications pass though an F5 Load-balancer running an ASM Web Application Firewall policy.

    The VIP for the web tier of the application lives on the F5 BIG-IP virtual appliance. The pools servicing this VIP are dynamically populated by querying Consul for all instances of the service called `web` <br>

    The mapping between Consul and the pools was declared in the AS3 declaration of the VIP.

    Next you can explore some of the service discovery capabilities of Consul.

    View the nodes and services via CLI

    ```
    consul members
    consul catalog services
    ```

    You can also retrieve information programatically via the API.  In this example you will be determining the IP address of the first web servers returned by the API. <br>

    ```
    web_server=$(curl -s $CONSUL_HTTP_ADDR/v1/catalog/service/web | jq -r '.[0].Address')
    echo $web_server
    ```

    ** NGiNX **

    Nginx is running on each of the web servers and is used to proxy and load balance requests to the app tier.

    Rather than managing the configuration files for the proxy manually, we'll instead rely on Consul and a library called [Consul Template](https://github.com/hashicorp/consul-template) <br>

    Consul Template is responsible for monitoring Consul for changes to the `app` service, and when changes are detected, render a new configuration file and restart NGiNX automatically.

    **Pro Tip:** Consul template can render any ASCII based configuration files you may need for other purposes, and fire a user defined handler after (re)rendering. <br>

    The next few steps will show how this is configured, by accessing the `$web_server` you discovered and stored previously through a bastion host that has also been configured. <br><br>

    1. Review the consul-template configuration
    ```
    ssh -q -A -J azure-user@$bastion_ip azure-user@$web_server \
      sudo cat /etc/consul-template/consul-template-config.hcl
    ```
    <br>

    2. Review the templated nginx configuration

    ```
    ssh -q -A -J azure-user@$bastion_ip azure-user@$web_server sudo cat /etc/nginx/conf.d/load-balancer.conf.ctmpl
    ```
    <br>
    3. Review the rendered configuration, and compare it to the information you've discovered via the other Consul interfaces.

    ```
    ssh -q -A -J azure-user@$bastion_ip azure-user@$web_server sudo cat /etc/nginx/conf.d/default.conf
    ```

    We are now able to connect things together using **Service Names** instead of IP addresses. In the next exercise we will scale the application and watch Consul take care of the rest! <br>
  notes:
  - type: text
    contents: |
      Now let's review the application environment we've deployed!
  tabs:
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  - title: App Access Info
    type: code
    hostname: workstation
    path: /info.txt
  - title: Shell
    type: terminal
    hostname: workstation
  - title: App
    type: service
    hostname: workstation
    path: /ui
    port: 9090
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/app
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/f5-hashicorp-app-mod-tf-consul/assets/diagrams/final-architecture.html
  difficulty: basic
  timelimit: 3000
- slug: 10-scale-the-application
  id: tjlwihpjo0s9
  type: challenge
  title: Scale the application
  teaser: Let Consul take care of routine adds/moves/changes of services instances.
  assignment: |-
    Now let's re-run the Terraform code for the app application, but pass in some new variables for the scale parameters.

    ```
    terraform apply -var app_count=3 -var web_count=3 -auto-approve
    ```

    After the Terraform run completes, you can monitor the status of your nodes and services using the Consul UI. Once all of the new instances are online and healthy, you can revisit some of the things we reviewed in the previous exercise.

    View the nodes and services via CLI

    ```
    consul members
    consul catalog services
    ```

    Review all of the web instances
    ```
    curl -s $CONSUL_HTTP_ADDR/v1/catalog/service/web | jq
    ```

    Review all of the app instances
    ```
    curl -s $CONSUL_HTTP_ADDR/v1/catalog/service/app | jq
    ```

    Review the updated nginx configuration on the web_server
    ```
    ssh -q -A -J azure-user@$bastion_ip azure-user@$web_server sudo cat /etc/nginx/conf.d/default.conf
    ```

    Review the VIP and pool configuration on LTM once again.

    The resources for this lab will self-destruct in 8 hours, but to save a little money, **please scale the application back down.**

    Re-run Terraform, and monitor the various integration points once again. We'll do so in the background so that you can move on whenever you're ready. <br>

    ```
    nohup terraform apply -var app_count=1 -var web_count=1 -auto-approve > /root/terraform/app/scaledown.out &
    ```

    **Some other things you can try:**

    1. Refresh the application page several times, and notice the load balancing occur.
    2. Explore the [Consul API](https://www.consul.io/api-docs/index) a bit more, Consul is a wonderful real-time source of truth for dynamic network environments.
    ```
    curl -s $CONSUL_HTTP_ADDR/v1/health/checks/web | jq
    curl -s $CONSUL_HTTP_ADDR/v1/health/checks/app | jq
    ```
    3. Consul can even estimate [round trip](https://www.consul.io/docs/internals/coordinates) times between nodes/datacenter. This is useful for finding the closest service instance, or datacenter for failover operations.
    ```
    consul rtt web-vm-000001 app-vm-000000
    ```
    **NOTE:** you may need to update the node names to match your environment

    Now that we are routing traffic based upon **service identity** we gain a lot of flexibility as more modern microservices architectures are adopted.
  notes:
  - type: text
    contents: |
      75% companies surveyed take Days or even Weeks to complete networking tasks.

      Organizations seeking to improve application delivery cycle are often blocked at the networking layer

      [source](https://zkresearch.com/research/2017-application-delivery-controller-study)
  tabs:
  - title: Current lab setup
    type: website
    url: https://htmlpreview.github.io/?https://raw.githubusercontent.com/hashicorp/field-workshops-consul/master/instruqt-tracks/f5-hashicorp-app-mod-tf-consul/assets/diagrams/final-architecture.html
  - title: Terraform Code
    type: code
    hostname: workstation
    path: /root/terraform/app
  - title: Cloud Consoles
    type: service
    hostname: workstation
    path: /
    port: 80
  - title: App
    type: service
    hostname: workstation
    path: /ui
    port: 8080
  - title: Vault
    type: service
    hostname: workstation
    path: /
    port: 8200
  - title: App Access Info
    type: code
    hostname: workstation
    path: /info.txt
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Consul
    type: service
    hostname: workstation
    path: /
    port: 8500
  difficulty: basic
  timelimit: 3000
checksum: "1413150568987970824"
